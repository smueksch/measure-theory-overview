%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Independence of events and independence of random variables %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Independence of Events and Random Variables}

\begin{theorem}{6.3}{Monotone Class Theorem}

    Let $\Pi$ be a $\pi$-system contained in a $\lambda$-system $\Lambda$. Then $\sigma(\Pi)$ is contained in $\Lambda$.

\end{theorem}

\begin{proposition}{6.4}{Extending $\pi$-System Independence}

    Let $C_1$ and $C_2$ be two \emph{independent} $\pi$-systems, i.e.

        \begin{align*}
            P(A \cap B) = P(A)P(B) \quad \forall A \in C_1, B \in C_2,
        \end{align*}

    then the \SigmaAlgebra s $\sigma(C_1)$ and $\sigma(C_2)$ are also independent.

\end{proposition}

\begin{theorem}{6.7}{Fubini-Tonelli Theorem}

    Let $(\Omega_i, \CalF_i, \mu_i)$, for $i = 1,2$, be measure spaces and $(\Omega, \CalF, \mu)$ be the product measure space of the two, i.e. $\Omega = \Omega_1 \times \Omega_2$, $\CalF = \CalF_i \otimes \CalF_2$ and $\mu = \mu_1 \otimes \mu_2$. Let $f: \Omega \to \overline{\mathbb{R}}$ be a \emph{non-negative} $\CalF$-measurable function. If $\mu_i$, for $i = 1,2$, are \emph{finite measures} on $\Omega_i$, for $i = 1,2$, respectively, then the following iterated integrals are well-defined and:

        \begin{align*}
            \int_{\Omega_1 \times \Omega_2} f \, d\mu_1 \otimes \mu_2 &= \int_{\Omega_1} \int_{\Omega_2} f \, d\mu_2 d\mu_1 = \\ &= \int_{\Omega_2} \int_{\Omega_1} f \, d\mu_1 d\mu_2.
        \end{align*}

    Furthermore, this statement holds for $\CalF$-measurable functions if:

        \begin{align*}
            \int_{\Omega_1 \times \Omega_2} |f| \, d\mu_1 \otimes \mu_2 < \infty.
        \end{align*}

\end{theorem}

\begin{lemma}{6.9}{Borel-Cantelli (Full)}

    Let $(A_n)_{n=1}^{\infty}$ be a sequence of sets and set

        \begin{align*}
            A \coloneqq \limsup_{n \to \infty} A_n \coloneqq \bigcap_{n=1}^{\infty}\bigcup_{k=n}^{\infty} A_k,
        \end{align*}

    then the following statements holds:

        \begin{enumerate}[(i)]
            \setlength{\parskip}{0em}
            \item If $\sum_{n=1}^{\infty} \mu(A_n) < \infty$, then $\mu(A) = 0$.
            \item If all $A_n$ are \emph{jointly independent} and $\sum_{n=1}^{\infty} P(A_n) = \infty$, then $P(A) = 1$.
        \end{enumerate}

    \Hint (i) provided in general case. (ii) Prove $P((\limsup_{n \to \infty} A_n)^C) = 1$, define $B_n = \bigcap_{k=n}^{\infty} A_k^C$ and show that for a given $P(B_n) = P(\lim_{m \to \infty} \bigcap_{k=n}^{m} A_k) = 0$ using independence and observation that $1 - P(A) \leq e^{-P(A)}$. Finally, use \emph{sub}-$\sigma$-additivity for $P(\bigcup_{n=1}^{\infty} B_n)$. \emph{Do not} attempt to argue through increasing sequences.
    
\end{lemma}

\begin{exercise}{}{Pulling Sum Through Variance}

    Let $(X_i)_{i=1}^{\infty}$ be a sequence of \emph{pairwise independent} random variables. Assume that $EX_i^2 < \infty$ for $i=1,2,\hdots,n$, then

        \begin{align*}
            \mathrm{Var}\left(\sum_{i=1}^n X_i\right) = \sum_{i=1}^n \mathrm{Var}\left(X_i\right).
        \end{align*}

\end{exercise}
